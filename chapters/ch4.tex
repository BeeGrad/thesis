\chapter{DEEP LEARNING METHODS}\label{deep_methods}

As with many different computer vision problems, deep learning methods have been used for image inpainting.
The reason why deep learning methods are being used more frequently as time progresses is that they provide more successful results in complex problems compared to traditional methods. The main reason for this is the creation of large-scale datasets that will enable the training of deep methods and the computational power that allow the training of these deep methods.

\section{CNN-based Methods}

Convolutional neural network structures, which are known to be very successful in computer vision studies thanks to their grid-like layer topology, are also used in image inpainting studies and give outstanding results. There are many architectures specially designed for inpainting work.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{U-Net Architecture}
    \label{fig:my_label}
\end{figure}

The U-Net [24] architecture used in Shift-Net inpainting [23] is one of them. This architecture takes an image and a mask which shows the missing areas and put them into convolutio layers. It concatenate the output of each layer with the corresponding layer of the same size in symmetrical architecture. The results of this structure achieve excellent success in terms of genereted image structure and fine detail.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Shift-Net Architecture}
    \label{fig:my_label}
\end{figure}

Another popular architecture used in inpainting studies is the encoder-decoder network. Sidorov and Hardeberg [5] uses a 3D CNN encoder-decoder network architecture and can perform not only inpainting but also tasks such as denoising and super-resolution. In the study of Liu et al. [6] coherent semantic attention layer designed and used in an encoder-decoder structure called refinement network. This architecture can be seen in figure 6.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{CSA layer at the resolution of 32×32 in refinement network}
    \label{fig:my_label}
\end{figure}

In Artist-Net [7] proposed by Liao et al. In order to achive inpainting, two different content encoders, a style encoder used with a joint decoder. A similar structure is also available in CNN architecture developed by Cai et al. [1] for the purpose of semantic object removal. There are also context encoder [2] architectures used in inpainting studies. These context encoders basically CNNs trying to generate the missing parts of an image from their surroundings.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Artist-Net Architecure}
    \label{fig:my_label}
\end{figure}

Zeng et al. [3] used Pyramid-context encoder network to perform inpainting. This pyramidal arthitecture called PEN-Net performs a high quality inpainting operation. The network structure can be seen in figure. Nakamura et al. [25] Aimed to remove the text from images and achieved excellent results using CNN.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{The Pyramid-context Encoder Network (PEN-Net)}
    \label{fig:my_label}
\end{figure}

\section{GAN-based Methods}

Bengü yazacak.

\section{State-of-the-Art Methods}

\subsection{EdgeConnect}

Bengü yazacak.

\subsubsection{Edge Generator}

Bengü yazacak.

\subsubsection{Image Completion Network}

Bengü yazacak.

\subsection{Generative Image Inpainting with Contextual Model}

Bengü yazacak.

\subsubsection{Contextual Attention}

Bengü yazacak.

\subsection{Image Inpainting via Generative Multi-column CNN}

In the article [ref:gmcnn], the inpainting process is presented with a multi column CNN structure. This network acquires different image features in parallel. A new reconstructon loss function has been developed to better characterize global objects. Also, a loss function called ID-MRF loss has been developed to increase the local details and improve the texture quality. With these loss functions and a adversarial loss, the model trained on the image dataset can fill the masked pixels in an image with local and global information. Trained model of this architecture shows realistic results without even the need for post-processing, which is often seen on similar works.

This inpainting system has a structure that can be trained from begining to end. It first receives an image and a binary mask as input. The pixels to be masked are denoted by 1 and known pixels by 0. Then the image masked by the  operation and inserted into the network. The result of the network is a completed image. Network structure is as shown in the figure 3.5.1.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Network structure of the GMCNN}
    \label{fig:my_label}
\end{figure}

The network structure shown in figure 3.5.1 consists of three sub-networks. The first one is the generator network that creates the output painted image and is the main network which will used after the training phase.The other two sub-networks are networks that are only needed during the training phase respectively, local and global discriminator networks which used to calculate the adversarial loss and a pre-trained VGG network [ref:simonyan] in order to calculate the designed ID-MRF loss. Generator network consists of n parallel encoder-decoder branches (in figure 3.5.1, n = 3). These structures consist of convolution layers that apply filters of different kernel sizes to the input tensor. These convolution layers obtain features at different frequency levels according to the kernel size. Next, these branches which have different levels of information about input image upsampled bilinearly to the original image size. Then these same size branches combined together and forms a feature map. Continued with two more convolution layers, this feature map is transformed into the image. This image is the generated output image of the network and it is used to calculate the network loss when compared to the ground truth image. Even though these branches seem to be independent from each other, they are actually affected by the backpropagation phase in the training process. This framework is different from the commonly used encoder-decoder structures. In an encoder-decoder network, every layer inherits its information from its previous layer. However, in this architecture, different structures complement each other instead of just inheriting. With these different representation levels, the inpainting process is carried out.

To calculate loss during model training, a mixture of three loss functions used. ID-MRF loss, reconstruction loss, and adversarial loss.

ID-MRF stands for implicit diversified Markov random fields. This loss function is used to train with a path similarity based approach within a VGG structure fed with generated results. This method, which is used only during the training phase, is a metric used to measure the similarity between the randomly selected patches between generated pixel areas and the ground truth area pixel areas. A simple similarity measure like cosine similarity can be used to compute this process. This metric measures the similarity between the generated content and the random selected nearest-neighbor patches. But using cosine similarity measurement has some limitations. As seen in the figure 3.5.2.a, the results often become like blurry textures and for large masking areas this can become a serious problem.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Cosine similarity vs. Relative similarity}
    \label{fig:my_label}
\end{figure}

In this article, in order to eliminate this problem, a relative similarity measurement has been developed and used instead of a direct similarity such as cosine similarity. As seen in figures 3.5.2.b and 3.5.2.c, the ground truth image pathches compared with the generated image patches. This relative similarity [ref: rmerc] equation can be seen in figure 3.5.3.

\begin{equation}
    buraya equation gelecek
\end{equation}

u(v, s) in the equation represents cosine similarity between patch v from generated area and patch s from ground truth area, the  and  values are constants, and  represents the features on the Lth layer of the pre-trained VGG network. To be more precise, let generated content denoted as  and  is the corresponding ground truth, for any layer L, ID-RMF loss can be calculated as equation in the figure 3.5.4.

\begin{equation}
    buraya equation gelecek
\end{equation}

The  is the normalized relative similarity as in the figure 3.5.5 and Z is normalization factor. r and s represents the patches from generated image and ground truth image respectively.

\begin{equation}
    buraya equation gelecek
\end{equation}

If all r patches in the  are close to only one spesific patch and far from other patches in ground truth, relative similarity becomes small and leads to a big loss value (L). On the other hand if every r patch from generated image has a similar corresponding s patch in ground truth image, relative similarity becomes big and leads to a small loss value. One of the biggest contributions of this measure is to increase the similarity between the  and  features. While minimizing the loss, generated feature distrubution and ground truth feature distrubution approach each other. As a result, in the inpainting regions blurry texture problem is overcome and variation is achieved.

\begin{equation}
    buraya equation gelecek
\end{equation}

With this loss active, during the training phase, a realistic texture was obtained both locally and globally. The features layers mentioned and shown in the figure 3.5.6 are taken from the VGG19 pre-trained model [ref:vgg].

A pixel-wise working reconstruction loss has been developed as a spacial variant reconstruction loss. In this designed confidence-driven reconstruction loss, known pixel values assigned with the value of 1 and others missing pixels confidence values gradually decays with respect to the distance from the mask border. Thus the pixels on the boundary make the most impact on the loss. As a result, a smooth transition between the masked area and the rest of the image is achieved. Increasing distance away from the boundary is obtained by convolving the mask with a gaussian filter and formulized like equation in the figure 3.5.7.

\begin{equation}
    buraya equation gelecek
\end{equation}

g represents a 64×64 gaussian filter with standart deviation of 40 and ◎ is the Hadamard product operator. Loss weight mask  is obtained by repeating the equation in figure 3.5.7 several times. Lastly the reconstruction loss calculated with the formula in the figure 3.5.8.

\begin{equation}
    buraya equation gelecek
\end{equation}

In figure 3.5.8, G([X,M];θ) represents the generated output. With the help of this loss, the results are softened from the mask boundaries towards the center.

Adversarial loss is used frequently in many tasks such as inpainting missing pixels. In this architecture, improved Wasserstein GAN [ref:gulrajani] is used as a generator and one local and one global structure are used together as a discriminator. Reason for using two discriminators is to overcome the problem when generated areas are realistic and low loss but ill-matched with the rest of the image. The adversarial loss function is defined as in figure 3.5.9, where X = tG([X, M]; θ) + (1 − t)Y and t  [0, 1].

\begin{equation}
    buraya equation gelecek
\end{equation}

Finally, the three loss functions mentioned above are combined together with different weights denoted as  and . Regularization of the model achieved in training process.

\begin{equation}
    buraya equation gelecek
\end{equation}

As a comparison with GMCNN, two other CNN-based inpainting architectures selected under same conditions with the same losses and same hyperparameters. A single encoder-decoder network and the previously mentioned contextual attention coarse network [ref:context] tested alongside with the GMCNN. Results are shown in figure 3.5.11.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{GMCNN Comparison}
    \label{fig:my_label}
\end{figure}

Also our implementation of the GMCNN using a pretrained network, achieved great results with two different datasets. Places [ref] dataset model and CelebA [ref] dataset model outputs can be seen in the figures 3.5.12 and 3.5.13 respectively.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Result of the model trained on Places2 dataset}
    \label{fig:my_label}
\end{figure}

As a limitation, like most of the generative neural networks this network also struggle with large datasets with diverse classes such as ImageNet. It is a harsh challange to inpaint different objects and places with one pre-trained model.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Result of the model trained on CelebAHQ dataset}
    \label{fig:my_label}
\end{figure}

\subsection{Deep Image Prior}

Deep convolutional networks have become very popular for image generation and restoration problems. In general, the reason behind the good results of such networks is the ability to obtain realistic image priors from datasets containing large number of images. To explain briefly what image prior is, it is a general definiton for the information on an image that can be used for any kind of image processing tasks to enhance results, to choose the processing parameters, and resolve indeterminacies. For instance, some information about an image may known such as color distrubution, and this information or its approximation can be used as a prior to a spesific task. Image priors can be represented with math form and merged into processing steps such as filtering, deconvolution and segmentation which helps to reduce the feasible solutions.

In the article [ref: deepimageprior], the authors show that a generator network structure can get enough low-level image statistics without a long training phase on any dataset. To prove this hypothesis, designed deep image prior method with a randomly-initialized convolutional neural network was used as a prior and highly competitive results were obtained in various inverse computer vision problems such as denoising, super-resolution, and image inpainting. What makes this method important except that it can be used in a wide range of cumputer vision research areas, it can also forms a new branch between learning-based networks and learning-free networks with fixed explicit priors like self-similarity etc. To be declared in a single sentence, in deep image prior, authors tries to bridge the gap between two popular methods by constructing a new explicit prior using convolution neural network.

To perform the task of image restoration like inpainting, learned-prior and explicit-prior are the two common methods utilized by researchers. To explain with an example, learned-prior is a straight forward approach to train a deep convolution network to learn about the world through the dataset. On the other hand, explicit-prior or hand-crafted prior method, is embedding constrains and distinctly teaching what types of images are natural. However in real word it is extremely difficult to express constraints mathematically. As a result, most of the explicit-prior methods works poorly compared to a state-of-the-art pre-trained neural network, yet deep image prior method offers surprisingly good results which can compete with learned-prior methods.

Deep convolutional networks are architectures that on of the best for solving inverse image problems. Architectures like generative adversarial networks and variational auto-encoders achieve state-of-the-art solutions on this tasks. However, it is not entirely correct to assume that the reason for the amazing results of these models trained on large datasets is that they can capture realistic image priors while learning over the data. Because only a good learning curve on a selected datasets does not prove that a network performs good on an image from a different dataset. A good network requires generalization which means that the structure of the network should resonate with the structure of the given input data. For example, as shown [1], an image classification network which successfully generalized for real data can overfit when a random labels are presented.

To put it more clearly, in this article the authors shows that learning process is not a requirement and it is possible to obtain quite good image priors with a convolutional generative structure that does not need training. In order to prove this claim, an untrained, randomly-initialized convolutional generator neural network is fitted on a given corrupted image. Generator network initialized with a random distrubution conditioned to the input corrupted image. The reconstruction task is expressed as a conditional image generation problem. Randomly initialized network weights optimized without needing any other data than input image and network itself. “Only prior information is in the structure of the network itself.” [] as authors states.

\begin{equation}
    buraya equation gelecek
\end{equation}

In figure 3.6.1, x is corrupted input image, z is a code tensor and  represents the network parameters which randomly initialized in the beginning and will map code tensor z to the image x while resolving inverse problem through iterations. The network is switching between filtering operations such as non-linear activation, convolution, and upsampling. For most of the experiments, a hourglass type U-Net architecture with two million parameters used.

In image restoration problems the goal is to recover original image x, when having a corrupted image. To solve such tasks, problem is often formulated as an optimization like in figure 3.6.2.

\begin{equation}
    buraya equation gelecek
\end{equation}

In equation 3.6.2, E(x;) is a data term dependent to the task and R(x) is an image prior. For a wide range of problems, such as super-resolution, denoising, and inpainting, the data term is typically simple to design, while designing the image prior is a challenging. For inpainting problems, the corresponding data term is defined as in the equation 3.6.3.

\begin{equation}
    buraya equation gelecek
\end{equation}

The symbol ◎ is the Hadamard product operator and m is the binary mask applied onto ground truth image x. The goal is to reconstruct the ground truth image x from the given image  with missing pixel values.

As mentioned, choosing a regularizer R(x) that can catch general priors in image x is a difficult and very extensive research subject within itself. To give an example, if total variation (TV) is selected as a regularizer, the results are often have uniform regions [ref: mahendran]. In this study, as seen in the equation 3.6.4, implicit prior which gathered from neural network was used as a regularizer.

\begin{equation}
    buraya equation gelecek
\end{equation}

* is the minimizer achieved with the help of the gradient descent optimizer from random parameter values. Result of the inpainting process denoted as x* is given in figure 3.6.4. It is also possible to apply minimazation on the code tensor z. However here z is a fixed three dimensional tensor containing 32 feature maps of uniform noise with the same spatial size as x.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Deep Image Prior Comparison}
    \label{fig:my_label}
\end{figure}

In the figure 3.6.5, deep image prior method compared with Shepard Neural Networks [ref:shepard] which is particularly designed for inpainting applications. Using a black text as a mask, this method demonstrates astonishing results. While in the output of the Shepard network remainings of the mask can be seen, in the output of the deep image prior method there are almost no artifacts.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Deep Image Prior output}
    \label{fig:my_label}
\end{figure}

In another experiment, deep image prior is compared with Convolutional Dictionary Learning method [ref: dictionary] using an image where as a mask half of the pixels are removed by applying a bernoulli distrubution with random noise. Visual comparison is given in figure 3.6.6.

Apart from the inpainting study of small pixel regions, a large region inpainting process was also tested and deep image prior method has shown great success again. Although deep image prior method works great for variety of images, as a limitation it works poorly while inpainting large masked areas of the images which have highly semantic information such as face images due to being a learning-free method.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Deep Image Prior output}
    \label{fig:my_label}
\end{figure}

Result of the inpainting of a large masked region with deep image prior method is compared with a learning-based generative adversarial network model [ref: global-local]. As shown in the figure 3.6.7, the pretrained GAN structure performs the inpainting process with the information learned from the dataset it trained on. On the other hand deep image prior fills the missing region with texture information learned from known regions.

As mentioned, deep image prior is a method which can be applied onto any network architecture, the results obtained are highly dependent on the network structure used. In figure 3.6.7, deep image prior method applied on different neural networks. From the results, it can be concluded that deeper networks have positive effect on the inpainting solution. However, having skip-connections which can significantly improve image recognition tasks shows extremely harmful behaviour in inpainting process as seen in figure 3.6.8.e and 3.6.8.f where skip-connectons added to the ResNet [ref] and U-Net [ref] architectures.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Skip Layer effects}
    \label{fig:my_label}
\end{figure}

Finally, we tested the Deep Image Prior method using a encoder-decoder network and achieved quite enough results. Figure 3.6.9 shows the output after 1000 iteration.

\begin{figure}[h]
    \centering
    \includegraphics{}
    \caption{Result of the 1000th iteration of an encoder-decoder network}
    \label{fig:my_label}
\end{figure}