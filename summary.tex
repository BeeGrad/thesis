There may be parts in the images with different features that are not desired. These undesired parts might be caused by aging of a photograph or these parts could include an object which is wanted to be removed from the image. Removing these parts and inpainting them with visually realistic features is both a research area and a commercial problem. There are many specialized tools for this purpose and use of these tools may also require professional knowledge.

The aim of the Image Inpainting with Deep Learning project is to examine the different structures in the pictures that fill these regions that are extracted from the pictures. For this purpose, many different work types were divided into groups and sample studies from these groups were compared. Inpainting methods for review purposes, are divided into two as traditional methods and deep methods. Although many methods were mentioned during our study only a few are chosen. Navier-strokes and fast-marching methods were chosen for comparison as example of traditional methods. EdgeConnect, Generative Contextual, GMCNN (ASIL ADI) and Deep Prior methods, which were found to give successful results among deep methods, were selected to be examined in detaial and implemented. In addition to all these, two novel methods are implemented which are inspired from all methods we examined throughout this project. Results of these two methods are also explained and examined in this project.

As examples of deep methods, our two methods, EdgeConnect and Generative Contextual are two GAN based studies which use deep artificial neural networks that work adversarial to each other. There are two stages in the EdgeConnect method and these two stages use the mentioned GAN structure. In the first stage, corrupted image is taken as input to the model and edge information is predicted, and in the second stage, it is desired to produce an image whose missing region is filled by using the both edge information and masked image. Also, in Generative Contextual study there are two stages that use GAN aswell. In the first stage, maske area is inpainted roughlt, and in the second stage, improvement operations are carried out on the created image by using a layer that aims to obtain contextual information.

% CNN Part

Pytorch and Tensorflow artificial intelligence libraries were used during our study. Our studies were run on the graphics card, especially since serious processing power was required for training and testing of deep learning methods. As a result of our project, different studies that we tested were compared and presented with PSNR and SSIM values and example images.